# GPU-ollama-RAG.py
This Python script ([`GPU-ollama-RAG.py`](https://github.com/morvayd/Ollama-docling-RAG/tree/main)) is a **Retrieval-Augmented Generation (RAG) chatbot** that uses local Ollama AI models.

**Main Features:**
- **Interactive AI Chat**: Command-line interface with streaming responses from locally-running Ollama LLM/SLM models
- **Document RAG**: Loads documents (PDFs, etc.) using IBM's Docling library, chunks them into 2048-word segments, and answers questions based on document content
- **Multiple Personalities**: Standard assistant, Pirate (Captain RedEye), Jeeves (butler), or Mystic (SageBrush) modes
- **Thinking Mode**: Optional display of AI reasoning process
- **Cross-platform**: Works on Windows, Linux, and macOS
- **Comprehensive Logging**: Tracks all interactions in SQLite database with metadata (tokens, timing, word counts)

**Key Commands:**
- `load`, `unload`, `text`, `chunk`, `think`, `pirate`, `jeeves`, `mystic`, `model`, `quit`

**Technical Stack:**
- Ollama API, Docling (document processing), Colorama (colored output), Pandas/SQLite (data tracking), custom PythonLog module

**Workflow:**
1. User selects an Ollama model from installed models
2. User can optionally load a document
3. User asks questions (with or without document context)
4. AI streams responses in real-time
5. All interactions logged to database and text files

Note:  The above summaries were auto generated by the IBM Project Bob AI.  

Thank you!

D. Morvay
